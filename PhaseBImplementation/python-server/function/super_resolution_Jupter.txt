# Set up for Colab: tf2.1 and PyDrive to download the dataset

# installing tensorflow 2.1
# !pip install --upgrade pip setuptools wheel

# !pip install protobuf==3.20.*

!pip install --quiet tensorflow-gpu==2.12.0

# After this, restart the runtime to avoid authentication issues
!pip install -q -U PyDrive

  error: subprocess-exited-with-error
  
  × python setup.py egg_info did not run successfully.
  │ exit code: 1
  ╰─> See above for output.
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  Preparing metadata (setup.py) ... error
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.

# Import PyDrive2 and associated libraries.
from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# Authenticate and create the PyDrive2 client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

#avishai adding
# Mount Google Drive if using Colab
try:
    from google.colab import drive as colab_drive
    colab_drive.mount('/content/drive', force_remount=True)
    COLAB = True
    print("Note: Using Google Colab")
    %tensorflow_version 2.x  # Assuming you want to switch to TensorFlow 2.x
except:
    print("Note: Not using Google Colab")
    COLAB = False

# Check the existence of the file
# file_path = '/content/drive/MyDrive/ProjectPhaseB/6_128.jpg'
# import os
# if os.path.exists(file_path):
#     print(f"File found at: {file_path}")
# else:
#     print(f"File not found at: {file_path}")
# from google.colab import drive
# drive.mount('/content/drive')

# Specifying the file ID
file_id = '19Q0BLy0jyxjZsa-4V_oM9Cw2QTIIlGeZ'
downloaded = drive.CreateFile({'id': file_id})

# Download the file to the Colab environment
downloaded.GetContentFile('Linnaeus_5_bigger.zip')

# Check the files in the current directory to confirm the zip file is downloaded
!ls

# Unzip the downloaded file
!unzip -q Linnaeus_5_bigger.zip

# List contents of the directory to confirm the extraction
!ls Linnaeus_5_bigger

# Perceptual loss function

# load pre-trained (imagenet) vgg network, excluding fully-connected layer on the top
vgg = VGG19(include_top=False, weights='imagenet', input_shape=(None,None,3))
vgg_layer = K.Model(inputs=vgg.input, outputs=vgg.get_layer('block3_conv3').output)
# make the net not trainable
for l in vgg_layer.layers: l.trainable=False

print(vgg_layer.summary())

def perceptual_loss(y_true,y_pred):
    '''This function computes the perceptual loss using an already trained VGG layer'''
    y_t=vgg_layer(y_true)
    y_p=vgg_layer(y_pred)
    loss=K.losses.mean_squared_error(y_t,y_p)
    return loss

def image_proc_net(training_mode=True):
    """
    Function which creates the model to preprocess images.
    2 different types of model with different layers depending
    on whether it is in training mode or in testing mode
    """
    inputs = K.layers.Input((None, None, 3))
    if training_mode:
        # training mode: downsampling, gaussian noise and upsampling
        x = K.layers.MaxPool2D(pool_size=(2,2))(inputs)
        x = K.layers.GaussianNoise(5)(x)
        x = K.layers.UpSampling2D((2,2))(x)
    else:
        # testing mode: just an upsampling
        x = K.layers.UpSampling2D((2,2))(inputs)

    model = K.models.Model(inputs, x)

    for l in model.layers: l.trainable=False

    return model

# istantiating the two models (for train and test)
image_proc_train = image_proc_net(training_mode=True)
image_proc_test = image_proc_net(training_mode=False)

# defining other metrics:
def psnr(y_true,y_pred):
    return tf.image.psnr(y_true,y_pred,1.0)
def ssim(y_true,y_pred):
    return tf.image.ssim(y_true,y_pred,1.0)

#training_mode
print(image_proc_train.summary())

def image_preprocess(image, training_mode=True):
    """
    Function which preprocess automatically an image
    wether it's in training or testing mode

    input:
      - image: np array (image tensor)
      - training_mode: binary flag
    """
    image = np.expand_dims(image,axis=0)
    if training_mode:
        return tf.squeeze(image_proc_train(image))
    else:
        return tf.squeeze(image_proc_test(image))

# reading image
def read_image(img_path):
    """
    function which loads an image from a path and convert it into np array
    """
    img = cv2.imread(img_path)
    b,g,r = cv2.split(img)   # cv2 reads BGR instead of canonical RGB
    img = cv2.merge([r,g,b]) # Switching it to RGB

    return img
import os
import numpy as np

all_images = []  # All train image paths

# 128x128 in training phase
path = 'Linnaeus_5_bigger/train_256'

# Check if the path exists
if not os.path.exists(path):
    print(f"Path does not exist: {path}")
else:
    files = os.listdir(path)
    for file in files:
        path_file = os.path.join(path, file)
        for dirpath, dirnames, filenames in os.walk(path_file):
            # Append full path to filenames
            filenames = [os.path.join(dirpath, x) for x in filenames]
            all_images.extend(filenames)

    # Picking only 2000 images
    all_images = all_images[:2000]
    np.random.shuffle(all_images)  # Random shuffling

    # Keep 85% for train and 15% for validation
    train_len = int(len(all_images) * 0.85)

    train_list = all_images[:train_len]
    val_list = all_images[train_len:]

    print(f"Total images: {len(all_images)}")
    print(f"Training images: {len(train_list)}")
    print(f"Validation images: {len(val_list)}")


# test
# for the test set we opted to take the same dataset
# but with images of size 64x64 instead of 128x128

test_list = []

path = 'Linnaeus_5_bigger/test_128'
files = os.listdir(path)
for file in files:
    path_file = path+'/'+file
    for (dirpath, dirnames, filenames) in os.walk(path_file):
        filenames = list(map(lambda x:path_file+'/'+x,filenames))
        test_list.extend(filenames)

np.random.shuffle(test_list) # random shuffling

test_list = test_list[:500]

print('images: ')
print('train:', len(train_list), 'val:', len(val_list), 'test:', len(test_list))


# defining train, validation and test generator

def train_generator():
    global batch_size
    while True:
        for start in range(0, len(train_list), batch_size):
                    x_batch = []
                    y_batch = []
                    end = min(start + batch_size, len(train_list))
                    ids_train_batch = train_list[start:end]
                    for i,ids in enumerate(ids_train_batch):
                        img_y = read_image(ids)
                        img_x = image_preprocess(img_y, training_mode=True)
                        x_batch.append(np.array(img_x,np.float32)/255.)
                        y_batch.append(np.array(img_y,np.float32)/255.)
                    x_batch = np.array(x_batch)
                    y_batch = np.array(y_batch)
                    yield x_batch,y_batch

def valid_generator():
    global batch_size
    while True:
        for start in range(0, len(val_list), batch_size):
                    x_batch = []
                    y_batch = []
                    end = min(start + batch_size, len(val_list))
                    ids_val_batch = val_list[start:end]
                    for i,ids in enumerate(ids_val_batch):
                        img_y = read_image(ids)
                        img_x = image_preprocess(img_y, training_mode=True)
                        x_batch.append(np.array(img_x,np.float32)/255.)
                        y_batch.append(np.array(img_y,np.float32)/255.)
                    x_batch = np.array(x_batch)
                    y_batch = np.array(y_batch)
                    yield x_batch,y_batch


def test_generator():
    global batch_size
    while True:
        for start in range(0, len(test_list), batch_size):
                    x_batch = []
                    y_batch = []
                    end = min(start + batch_size, len(test_list))
                    ids_test_batch = test_list[start:end]
                    for i,ids in enumerate(ids_test_batch):
                        img_y = read_image(ids)
                        img_x = image_preprocess(img_y, training_mode=False)
                        x_batch.append(np.array(img_x,np.float32)/255.)
                        y_batch.append(np.array(img_y,np.float32)/255.)
                    x_batch = np.array(x_batch)
                    y_batch = np.array(y_batch)
                    yield x_batch,y_batch

#blocks definition for upscaling/downscaling

def add_down_block(x_inp, filters, kernel_size=(3, 3), padding="same", strides=1,r=False):
    x = K.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides)(x_inp)
    x = K.layers.BatchNormalization()(x)
    x = K.layers.Activation('relu')(x)
    x = K.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides)(x)
    x = K.layers.BatchNormalization()(x)
    if r:
        # if r=True then we import an (1X1) Conv2D after input layer
        # in order the dimensions of 2 tensors coincide.
        x_inp = K.layers.Conv2D(filters,(1,1), padding=padding, strides=strides)(x_inp)
    x = K.layers.Add()([x,x_inp])
    return x

def add_up_block(x_inp, skip, filters, kernel_size=(3, 3), padding="same", strides=1, upscale_factor=2):
    # Calculate the upsampling factor based on the difference in input sizes
    upsampling_factor = skip.shape[1] // x_inp.shape[1]

    x = K.layers.UpSampling2D(size=(upsampling_factor, upsampling_factor))(x_inp)

    # Adjust the shape of the skip connection tensor
    skip = K.layers.Conv2D(filters, kernel_size=(1, 1), padding="same")(skip)

    x = K.layers.Concatenate()([x, skip])
    x = K.layers.BatchNormalization()(x)
    x = K.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides)(x)
    x = K.layers.Activation('relu')(x)
    x = K.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides)(x)
    x = K.layers.Activation('relu')(x)
    return x

def add_bottleneck(x_inp,filters, kernel_size=(3, 3), padding="same", strides=1):
    x = K.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides)(x_inp)
    x = K.layers.Activation('relu')(x)
    return x

def RUNet(input_size_train=256, input_size_test=128):
    """
    Implementing with Keras the Robust UNet Architecture as proposed by
    Xiaodan Hu, Mohamed A. Naiel, Alexander Wong, Mark Lamm, Paul Fieguth
    in "RUNet: A Robust UNet Architecture for Image Super-Resolution"
    """
    inputs = K.layers.Input((input_size_train, input_size_train, 3))

    down_1 = K.layers.Conv2D(64, (7, 7), padding="same", strides=1)(inputs)
    down_1 = K.layers.BatchNormalization()(down_1)
    down_1 = K.layers.Activation('relu')(down_1)

    down_2 = K.layers.MaxPool2D(pool_size=(2, 2))(down_1)
    down_2 = add_down_block(down_2, 64)
    down_2 = add_down_block(down_2, 64)
    down_2 = add_down_block(down_2, 64)
    down_2 = add_down_block(down_2, 128, r=True)

    down_3 = K.layers.MaxPool2D(pool_size=(2, 2), strides=2)(down_2)
    down_3 = add_down_block(down_3, 128)
    down_3 = add_down_block(down_3, 128)
    down_3 = add_down_block(down_3, 128)
    down_3 = add_down_block(down_3, 256, r=True)

    down_4 = K.layers.MaxPool2D(pool_size=(2, 2))(down_3)
    down_4 = add_down_block(down_4, 256)
    down_4 = add_down_block(down_4, 256)
    down_4 = add_down_block(down_4, 256)
    down_4 = add_down_block(down_4, 256)
    down_4 = add_down_block(down_4, 256)
    down_4 = add_down_block(down_4, 512, r=True)

    down_5 = K.layers.MaxPool2D(pool_size=(2, 2))(down_4)
    down_5 = add_down_block(down_5, 512)
    down_5 = add_down_block(down_5, 512)
    down_5 = K.layers.BatchNormalization()(down_5)
    down_5 = K.layers.Activation('relu')(down_5)

    bn_1 = add_bottleneck(down_5, 1024)
    bn_2 = add_bottleneck(bn_1, 512)

    up_1 = add_up_block(bn_2, down_5, 512, upscale_factor=input_size_train // input_size_test)
    up_2 = add_up_block(up_1, down_4, 384, upscale_factor=2)
    up_3 = add_up_block(up_2, down_3, 256, upscale_factor=2)
    up_4 = add_up_block(up_3, down_2, 96, upscale_factor=2)

    up_5 = pixel_shuffle(scale=2)(up_4)
    up_5 = K.layers.Concatenate()([up_5, down_1])
    up_5 = K.layers.Conv2D(99, (3, 3), padding="same", strides=1)(up_5)
    up_5 = K.layers.Activation('relu')(up_5)
    up_5 = K.layers.Conv2D(99, (3, 3), padding="same", strides=1)(up_5)
    up_5 = K.layers.Activation('relu')(up_5)

    outputs = K.layers.Conv2D(3, (1, 1), padding="same")(up_5)
    model = K.models.Model(inputs, outputs)
    return model

model = RUNet()
model.summary()

opt=K.optimizers.Adam(learning_rate=0.001) # Adam optimizer
model.compile(optimizer=opt,loss=perceptual_loss,metrics=[psnr,ssim,K.losses.mean_squared_error])
history = model.fit_generator(generator=train_generator(),
                              steps_per_epoch=np.ceil(float(len(train_list)) / float(batch_size)),
                              epochs=20,
                              verbose=1,
                              validation_data=valid_generator(),
                              shuffle=True,
                              validation_steps=np.ceil(float(len(val_list)) / float(batch_size)))

def history_results(history, par1='loss', par2='val_loss', title='loss'):
    """
    Plot the history of the the 2 metrics (par1, par2) during
    the training (epochs)
    """
    plt.plot(history.history[par1])
    plt.plot(history.history[par2])
    plt.title(title)
    plt.ylabel(par1)
    plt.xlabel('epoch')
    plt.legend(['train', 'val'], loc='upper left')
    plt.grid()
    plt.show()

    return
# print(history)
import os

path = '/content/drive/MyDrive/ProjectPhaseB/super_resolution_model.h5'

# Ensure the directory exists, create it if necessary
directory = os.path.dirname(path)
if not os.path.exists(directory):
    os.makedirs(directory)


# Assuming 'model' is your trained model
# model.save_weights(path)
model.save(path)  # Saves the model in HDF5 format
print(f"Model saved to '{path}'.")

def show_pictures(img_idx, x_batch, y_batch):
    """
    Function which shows 3 images:
    - Ground truth: High Resolution image
    - Low Resolution image
    - Super Resolution image using our trained model
    """
    fig = plt.figure(figsize=(15,18))

    ax1 = fig.add_subplot(1,3,1)
    im = model(np.expand_dims(x_batch[img_idx],axis=0))
    im = np.squeeze(im)
    #ax1.imshow((abs(im) * 255).astype(np.uint8))
    ax1.imshow(abs(im))
    ax1.set_title('Super Resolution (from LR)')

    ax2 = fig.add_subplot(1,3,2)
    #ax2.imshow(x_batch[img_idx] * 255).astype(np.uint8))
    ax2.imshow(x_batch[img_idx])
    ax2.set_title('Low Resolution')

    ax3 = fig.add_subplot(1,3,3)
    #ax3.imshow((y_batch[img_idx] * 255).astype(np.uint8))
    ax3.imshow(y_batch[img_idx])
    ax3.set_title('Ground truth')

    return

    # trying to visualize the results with some training images

chunk_size=20

x_chunk = []
y_chunk = []

ids_train_chunk = train_list[0:chunk_size]
for i,ids in enumerate(ids_train_chunk):
    img_y = read_image(ids)
    img_x = image_preprocess(img_y)
    x_chunk.append(np.array(img_x,np.float32)/255.)
    y_chunk.append(np.array(img_y,np.float32)/255.)
x_chunk = np.array(x_chunk)
y_chunk = np.array(y_chunk)

import os
import cv2
import numpy as np

def read_image(img_path):
    """
    Reads an image from a file path.
    Converts from BGR to RGB format.
    """
    img = cv2.imread(img_path)
    if img is None:
        print(f"Error: Unable to read image at {img_path}")
        return None
    b, g, r = cv2.split(img)   # cv2 reads BGR instead of canonical RGB
    img = cv2.merge([r, g, b]) # Switching it to RGB
    return img

def get_image_paths(root_dir, size):
    """
    Collects image paths from the directory and filters by size.
    """
    image_paths = []
    for dirpath, _, filenames in os.walk(root_dir):
        for filename in filenames:
            if size in filename:  # Assuming size (e.g., '64' or '256') is in the filename
                image_paths.append(os.path.join(dirpath, filename))
    return image_paths

# Assuming image_preprocess is defined elsewhere

# Get all 256x256 image paths for training
train_root_dir = 'Linnaeus_5_bigger/train_256'
train_size = '256'
train_list = get_image_paths(train_root_dir, train_size)

# Get all 128x128 image paths for testing
test_root_dir = 'Linnaeus_5_bigger/test_128'
test_size = '128'
test_list = get_image_paths(test_root_dir, test_size)

# Prepare training data
x_train_chunk = []
y_train_chunk = []
train_chunk_size = 10  # Adjust the chunk size as needed
ids_train_chunk = train_list[:train_chunk_size]

for i, ids in enumerate(ids_train_chunk):
    # Construct the path for the corresponding 256x256 image
    ids_256 = ids.replace(train_size, train_size)
    print(f"Processing: 256x256 image at {ids_256}")

    # Check if the 256x256 image file exists
    if not os.path.isfile(ids_256):
        print(f"256x256 image file does not exist: {ids_256}")
        continue

    img_x = read_image(ids_256)     # x: 256x256 img
    if img_x is None:
        print(f"Skipping image at {ids_256}")
        continue

    img_x = image_preprocess(img_x, training_mode=False)

    x_train_chunk.append(np.array(img_x, np.float32) / 255.)
    y_train_chunk.append(np.array(img_x, np.float32) / 255.)  # Same image for both x and y for training

x_train_chunk = np.array(x_train_chunk)
y_train_chunk = np.array(y_train_chunk)

print(f"Processed {len(x_train_chunk)} training image pairs.")

# Prepare testing data
x_test_chunk = []
y_test_chunk = []
test_chunk_size = 10  # Adjust the chunk size as needed
ids_test_chunk = test_list[:test_chunk_size]

for i, ids in enumerate(ids_test_chunk):
    # Construct the path for the corresponding 128x128 image
    ids_128 = ids.replace(test_size, test_size)
    print(f"Processing: 128x128 image at {ids_128}")

    # Check if the 128x128 image file exists
    if not os.path.isfile(ids_128):
        print(f"128x128 image file does not exist: {ids_128}")
        continue

    img_x = read_image(ids_128)     # x: 128x128 img
    if img_x is None:
        print(f"Skipping image at {ids_128}")
        continue

    img_x = image_preprocess(img_x, training_mode=False)

    x_test_chunk.append(np.array(img_x, np.float32) / 255.)
    y_test_chunk.append(np.array(img_x, np.float32) / 255.)  # Same image for both x and y for testing

x_test_chunk = np.array(x_test_chunk)
y_test_chunk = np.array(y_test_chunk)

print(f"Processed {len(x_test_chunk)} testing image pairs.")


#this is the resote image slot 
import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Model
from tensorflow.keras.models import load_model
from tensorflow.keras.optimizers import Adam

def RUNet():
    # Define your RUNet model architecture here
    return model



def load_model_super_resolution():
    path = '/content/drive/MyDrive/ProjectPhaseB/super_resolution_model.h5'
    # Load the model with custom loss and metrics
    custom_objects = {
        'perceptual_loss': perceptual_loss,
        'psnr': psnr,
        'ssim': ssim
    }
    model = load_model(path, custom_objects=custom_objects)
    return model

def super_resolve_and_display(image_path, save_path):
    model = load_model_super_resolution()

    # Preprocess the image
    low_res_img = preprocess_image(image_path)

    # Use the model to predict the high-resolution version of the image
    high_res_img = model.predict(np.expand_dims(low_res_img, axis=0))
    high_res_img = np.clip(high_res_img.squeeze(), 0, 1)  # Post-process to ensure it's displayable

    # Display the results
    plt.figure(figsize=(12, 6))
    plt.subplot(1, 2, 1)
    plt.imshow(low_res_img.squeeze())
    plt.title('Low Resolution')
    plt.axis('off')

    plt.subplot(1, 2, 2)
    plt.imshow(high_res_img)
    plt.title('High Resolution')
    plt.axis('off')
    plt.show()

    # Save the high-resolution image
    high_res_img = (high_res_img * 255).astype(np.uint8)
    cv2.imwrite(save_path, cv2.cvtColor(high_res_img, cv2.COLOR_RGB2BGR))
    print(f"High-resolution image saved to {save_path}")

# Additional functions assumed to be defined:
def preprocess_image(image_path):
    """
    Load an image and preprocess it for the model.
    """
    img = cv2.imread(image_path, cv2.IMREAD_COLOR)
    img = cv2.resize(img, (256, 256))  # Example resizing, adjust according to your model's input requirements
    img = img / 255.0
    return img

# Example usage
image_path = '/content/drive/MyDrive/ProjectPhaseB/12_128.jpg'
save_path = '/content/drive/MyDrive/ProjectPhaseB/12_high_res.jpg'
super_resolve_and_display(image_path, save_path)
